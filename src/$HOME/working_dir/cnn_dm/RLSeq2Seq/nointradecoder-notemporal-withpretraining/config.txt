vocab_size	50000
E2EBackProp	False
vocab_path	$HOME/data/cnn_dm/finished_files/vocab
dqn_scheduled_sampling	False
dqn_replay_buffer_size	100000
data_path	$HOME/data/cnn_dm/finished_files/chunked/train_*
convert_to_reinforce_model	False
use_temporal_attention	False
dqn_layers	512,256,128
dqn_pretrain_steps	10000
trunc_norm_init_std	0.0001
fixed_eta	False
dqn_gpu_num	0
ac_training	False
reward_function	rouge_l/f_score
calculate_true_q	False
intradecoder	False
restore_best_model	False
fixed_sampling_probability	False
exp_name	nointradecoder-notemporal-withpretraining
enc_hidden_dim	256
decay_function	linear
gpu_num	0
max_dec_steps	100
dueling_net	True
dec_hidden_dim	256
cov_loss_wt	1.0
lr	0.15
greedy_scheduled_sampling	False
rand_unif_init_mag	0.02
dqn_pretrain	False
log_root	$HOME/working_dir/cnn_dm/RLSeq2Seq/nointradecoder-notemporal-withpretraining
max_enc_steps	400
dqn_polyak_averaging	True
max_iter	260298
dqn_target_update	10000
batch_size	32
adagrad_init_acc	0.1
dqn_batch_size	100
coverage	False
max_grad_norm	2.0
alpha	1
convert_to_coverage_model	False
min_dec_steps	35
dqn_sleep_time	2
single_pass	False
matrix_attention	False
pointer_gen	True
eta	3.71368423007e-06
k	1
rl_training	True
emb_dim	128
beam_size	4
hard_argmax	True
mode	train
embedding	None
scheduled_sampling	False
debug	False
decode_after	0
gamma	0.99
sampling_probability	0
